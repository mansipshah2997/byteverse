version: 0.0.3
jobId: "376"
jobName: pyspark_extra_params_Mansi_again
jobType: Source Aligned Data Product
domain: sale
alias: write_del
discoveryPort:
  name: pyspark_extra_params_Mansi_again
inputPorts:
  - alias: weather_data_Mansi_1
    isDynamic: true
    path: s3://byte-etl-externaldemo/weather_data/20230626130122.csv
    optional:
      persistDataFrame: false
      advanceOptions:
        delimiter: ","
      enableDataReconciliation: false
      enforceSchema: false
      connection: Mansi_S3
      dataSetUrn: urn:dv:dataset:7ea0f9b9-1b2b-4d8c-a3b6-2a68936e1ad9
    type: inputDelimited
productState:
  persistDataFrame: false
  enableDataReconciliation: false
  enforceSchema: false
  stepName: write del
  path: s3://byte-etl-externaldemo/custom_pyspark_output/weather_data_daily_test.csv
  type: inputDelimited
  isStateManagement: true
  sequence: 3
  alias: write_del
  refreshInterval: None
  retentionVersions: ""
  logicalSchema: ""
  enforceSchemaMethod: ""
  isProfilingEnabled: false
transformation:
  - isDynamic: true
    alias: EMR_PySpark_1
    description: Pyspark Job
    sequence: 2
    arguments:
      - s3://byte-etl-externaldemo/weather_data/20230626130122.csv
    pythonFilePath: s3://bp-spark-sql-library-test-acc/custom-jobs/CustomPythonJobWriteParquetTimeOut.py
    inputDataFrameList:
      - inputDataFrame: ""
        tempViewName: ""
    optional:
      pythonEnvTarGZPath: s3://byte-etl-externaldemo/pyspark_serverless_test/pyspark_venv.tar.gz
      sparkExtraConf:
        spark.sql.legacy.timeParserPolicy: LEGACY
    type: customPySparkEMRServerless
controlPort:
  dataQualityRules: {}
outputPort:
  subscriptionChannels:
    - channelType: Postgres
      queryType: SQL
    - channelType: Dataproduct
      queryType: SQL
